# Sprint 7: RAGAS Evaluation Summary

## Execution Date
January 5, 2026

## Evaluation Configuration
- **Questions**: 10 golden questions covering all policy domains
- **RAGAS Version**: 0.4.2
- **Metrics**: Faithfulness, Answer Relevancy, Context Precision
- **LLM**: GPT-4o (for metrics), GPT-3.5-turbo (for RAG responses)
- **Processing Time**: ~4 minutes
- **Platform**: Windows 10 with Python 3.12

---

## Results Summary

### Overall Performance
| Metric | Score | Target | Status |
|--------|-------|--------|--------|
| **Faithfulness** | 0.5750 | >0.70 | ‚ùå NEEDS IMPROVEMENT |
| **Answer Relevancy** | 0.8413 | >0.70 | ‚úÖ GOOD |
| **Context Precision** | 0.0979 | >0.85 | ‚ùå CRITICAL |
| **Overall Average** | 0.5047 | >0.75 | ‚ùå NEEDS IMPROVEMENT |

---

## Detailed Analysis

### ‚úÖ Answer Relevancy: 0.84 (STRONG)
**What it measures**: How relevant and on-topic the generated answer is to the user's question

**Findings**:
- System generates highly relevant responses
- Query understanding works well
- Responses are focused and address user intent appropriately
- **This is the system's strongest metric**

**Conclusion**: ‚úÖ No action needed - maintain current approach

---

### ‚ö†Ô∏è Faithfulness: 0.58 (MODERATE CONCERN)
**What it measures**: Whether the answer is grounded in the retrieved context (prevents hallucination)

**Findings**:
- System occasionally extrapolates beyond source documents
- Some responses include information not strictly present in retrieved chunks
- Dropped from 0.75 (1-question test) to 0.58 (10-question evaluation)
- Indicates inconsistent grounding across different topics

**Recommended Actions**:
1. **Strengthen system prompt** - Add explicit "stay faithful to context" instructions
2. **Test GPT-4o** for response generation instead of GPT-3.5-turbo
3. **Add citation verification** - Validate claims against retrieved chunks
4. **Review problematic questions** - Analyze which questions caused low scores

---

### ‚ùå Context Precision: 0.10 (CRITICAL BOTTLENECK)
**What it measures**: Whether the retrieval system finds all relevant information from the knowledge base

**Findings**:
- **Only finding ~10% of relevant information**
- This is the PRIMARY bottleneck limiting overall system quality
- Hybrid search (dense + sparse vectors) + query expansion + reranking underperforming
- Even excellent response generation can't overcome poor retrieval

**Root Cause Analysis**:
- Query expansion may not be generating optimal search terms
- Hybrid search weights (dense vs sparse) may be imbalanced
- Reranking formula (70% similarity, 20% priority, 10% recency) may need tuning
- Initial retrieval limit may be too restrictive

**Recommended Actions** (Priority Order):
1. **URGENT**: Analyze failed retrievals - Identify which chunks should have been retrieved but weren't
2. **Tune hybrid search parameters** - Experiment with dense/sparse weights
3. **Optimize reranking formula** - Test different weight combinations
4. **Increase initial retrieval limit** - Fetch more candidates before reranking
5. **Review query expansion strategy** - Analyze quality of generated search terms
6. **Consider alternative approaches** - BM25 tuning, semantic chunking review

---

## Technical Challenges Resolved

### 1. Windows SSL Certificate Hang
- **Issue**: Script hung at `ssl.create_default_context()` on Windows
- **Root Cause**: HTTPX slow at certificate chain scanning on Windows
- **Solution**: Disabled SSL verification for localhost (Qdrant + OpenAI clients)
- **Files Modified**: `src/database/vector_store.py`, `scripts/evaluate_rag.py`

### 2. Unicode Encoding Errors
- **Issue**: Script crashed with Unicode emojis (‚è≥, ‚úì, üöÄ, üìä) when output redirected
- **Root Cause**: Windows terminal encoding limitations
- **Solution**: Replaced all emojis with ASCII equivalents ([*], [OK], [X], [!])
- **Files Modified**: `scripts/evaluate_rag.py`

### 3. Double Query Expansion Bug
- **Issue**: RAG Engine was expanding queries twice (redundant API calls)
- **Root Cause**: `query()` method called `expand_query()`, then `retrieve()` called it again
- **Solution**: Added `expanded_terms` parameter to avoid re-expansion
- **Files Modified**: `src/engine/rag_engine.py`

### 4. Async Event Loop Issues
- **Issue**: RLock recursion errors during multiprocessing
- **Solution**: Moved `nest_asyncio.apply()` to absolute first lines
- **Status**: Harmless cleanup error still occurs at shutdown (Python 3.12 + Windows quirk)

---

## Question Coverage

1. ‚úÖ SGLT2 inhibitors (Dapagliflozin) for heart failure - **Local CPICS policy**
2. ‚úÖ Continuous glucose monitoring eligibility - **Local + NICE guidelines**
3. ‚úÖ Rights when surgery is cancelled - **NHS Constitution**
4. ‚úÖ Individual Funding Request process - **Local ICB commissioning policy**
5. ‚úÖ Tirzepatide for type 2 diabetes - **Local CPICS + NICE**
6. ‚úÖ Insulin needles prescribing guidelines - **Local formulary**
7. ‚úÖ Dapagliflozin for chronic kidney disease - **Local CPICS + NICE TA775**
8. ‚úÖ Diabetes monitoring technology availability - **Local + NICE NG28**
9. ‚úÖ NHS Constitution key principles - **Legal/governance document**
10. ‚úÖ IFR standard operating procedures - **Local ICB governance**

---

## Processing Statistics

- **Total Duration**: ~4 minutes
- **API Calls**: ~70 total (query expansion, embeddings, retrieval, generation, metrics)
- **Rate Limiting**: Hit OpenAI 429 errors multiple times (automatically retried)
- **Timeouts**: 2 metric jobs timed out but didn't prevent completion
- **Success Rate**: 100% (10/10 questions, 30/30 metrics calculated)
- **Output File**: `logs/ragas_evaluation_results.json` (complete breakdown)

---

## Next Steps

### Immediate Actions (Sprint 8 Priorities)
1. **PRIORITY 1**: Optimize retrieval pipeline to improve Context Precision (0.10 ‚Üí 0.85)
   - Analyze failed retrievals
   - Tune hybrid search and reranking parameters
   - Increase retrieval candidates

2. **PRIORITY 2**: Improve Faithfulness to reduce hallucination (0.58 ‚Üí 0.70)
   - Strengthen system prompt
   - Test GPT-4o for generation
   - Add citation verification

3. **PRIORITY 3**: Maintain Answer Relevancy (keep above 0.70)
   - Monitor during optimizations
   - Ensure changes don't degrade this metric

### Future Enhancements
- Expand golden question set to 20-30 questions for statistical significance
- Automate periodic evaluations to track improvements
- Build Streamlit dashboard for results visualization
- Implement A/B testing for different configurations

---

## Files Updated

### Documentation
- ‚úÖ `Sprints/Sprint_7_Evaluation/Sprint_7_COMPLETE.md` - Full results, Windows fixes, lessons learned
- ‚úÖ `Sprints/00_Sprint_Master_Tracker.md` - Updated with actual baseline metrics
- ‚úÖ `Sprints/Sprint_7_Evaluation/EVALUATION_SUMMARY.md` - This file (executive summary)

### Code
- ‚úÖ `scripts/evaluate_rag.py` - Complete evaluation script with Windows compatibility
- ‚úÖ `src/database/vector_store.py` - Windows SSL fix for Qdrant client
- ‚úÖ `src/engine/rag_engine.py` - Fixed double query expansion bug

### Results
- ‚úÖ `logs/ragas_evaluation_results.json` - Complete 10-question evaluation results

---

## Conclusion

Sprint 7 successfully established a baseline evaluation framework and identified the critical bottleneck: **Context Precision (0.10)**. The retrieval system is the primary area requiring optimization. Answer Relevancy is strong (0.84), indicating good query understanding, but the system can't generate high-quality responses without first retrieving the right information.

**Overall Assessment**: System is functional but requires retrieval pipeline optimization before production deployment. The evaluation framework is operational and can now be used to measure improvements during Sprint 8 optimization work.

---

**Evaluation Complete**: January 5, 2026
**Next Sprint**: Sprint 8 - Optimization & Polish

