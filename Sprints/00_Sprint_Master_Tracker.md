# NEPPA Sprint Master Tracker

This document tracks the progress of all 8 sprints for the NHS Expert Patient Policy Assistant (NEPPA) project.

## Project Overview
- **Total Sprints**: 8
- **Target Duration**: 2 weeks (1.5-2 days per sprint)
- **Architecture**: Modular Monolith (FastAPI + Streamlit)
- **Python Version**: 3.11+

---

## Sprint Status Summary

| Sprint | Name | Status | Progress | Duration | Notes |
|--------|------|--------|----------|----------|-------|
| S1 | Ingestion | ‚úÖ Complete | 100% | - | PDF/DOCX parsing with metadata |
| S2 | Indexing | ‚úÖ Complete | 100% | - | Vector DB setup (Qdrant) |
| S3 | Logic | ‚úÖ Complete | 100% | - | RAG Prompt Engineering |
| S4 | MVP | ‚úÖ Complete | 100% | - | Interactive Interface (FastAPI + Streamlit) |
| S5 | Evidence | ‚úÖ Complete | 100% | - | Citation UI & Traceability |
| S6 | API | ‚è≥ Pending | 0% | - | Clinical Trials Integration |
| S7 | Eval | ‚úÖ Complete | 100% | - | RAGAS Testing |
| S8 | Polish | ‚è≥ Pending | 0% | - | Final UI & Documentation |

**Legend**: ‚úÖ Complete | üöß In Progress | ‚è≥ Pending

---

## Detailed Sprint Breakdown

### Sprint 1: Ingestion ‚úÖ
- **Goal**: Parse PDF/DOCX + Metadata
- **Deliverable**: Clean JSON/Text chunks with source tags
- **Status**: ‚úÖ Complete
- **Details**: See `Sprint_1_Ingestion/Sprint_1_COMPLETE.md`
- **Key Deliverables**:
  - ‚úÖ PDF/DOCX parsing (PyMuPDF, python-docx)
  - ‚úÖ Metadata tagging (source_type, organization, clinical_area, sortable_date, priority_score, is_presentation)
  - ‚úÖ Enhanced organization detection (text scanning for Governance documents)
  - ‚úÖ Slide-aware PowerPoint handling (page-based chunking)
  - ‚úÖ Recency & Authority logic (priority scoring for retrieval)
  - ‚úÖ 11 documents processed (505,749 chars, 76,650 words, 755 chunks)
  - ‚úÖ JSON output with structured metadata

### Sprint 2: Indexing ‚úÖ
- **Goal**: Vector DB Setup (Qdrant) with Enterprise Grade Hybrid Search
- **Deliverable**: Indexed knowledge base with Hybrid Search and Custom Reranking
- **Status**: ‚úÖ Complete
- **Details**: See `Sprint_2_Indexing/Sprint_2_COMPLETE.md`
- **Key Deliverables**:
  - ‚úÖ Qdrant Docker setup with persistence
  - ‚úÖ Collection "nhs_expert_policy" created with hybrid search schema (dense + sparse vectors)
  - ‚úÖ All 761 chunks embedded: Dense (OpenAI text-embedding-3-small) + Sparse (FastEmbed BM25)
  - ‚úÖ Payload indexes created (source_type, priority_score, clinical_area, organization)
  - ‚úÖ Hybrid search implemented with Reciprocal Rank Fusion (RRF)
  - ‚úÖ Custom reranking with multi-factor scoring (70% similarity, 20% priority, 10% recency)
  - ‚úÖ Local policy prioritization in reranking (Local=1.0, National=0.8)
  - ‚úÖ Batch upsert script and verification script with hybrid search

### Sprint 3: Logic ‚úÖ
- **Goal**: RAG Prompt Engineering - Expert Reasoning Engine
- **Deliverable**: Complete RAG engine with query expansion, hybrid retrieval, and expert reasoning
- **Status**: ‚úÖ Complete
- **Details**: See `Sprint_3_Logic/Sprint_3_COMPLETE.md`
- **Key Deliverables**:
  - ‚úÖ Query expansion using OpenAI GPT-3.5-turbo (3 clinical search terms)
  - ‚úÖ Hybrid retrieval integration with QdrantVectorStore
  - ‚úÖ Context synthesis with source metadata formatting and citation hints
  - ‚úÖ NICE reference code extraction (NG28, TA123, etc.) from filenames and chunk text
  - ‚úÖ Year extraction from date strings for citations
  - ‚úÖ Expert system prompt with clinical guardrails (Local > National hierarchy)
  - ‚úÖ Harvard-style citations ((CPICS, 2024), (NICE, NG28))
  - ‚úÖ Formal bibliography generation with structured format
  - ‚úÖ Clinical Governance & Authority section in responses
  - ‚úÖ Safety refusal triggers for out-of-scope queries
  - ‚úÖ Safety disclaimer footer automatically appended
  - ‚úÖ Response generation with professional clinical report format
  - ‚úÖ Test scripts validate end-to-end pipeline and robustness

### Sprint 4: MVP ‚úÖ
- **Goal**: Interactive Interface - End-to-End Chat Loop
- **Deliverable**: Working Prototype: FastAPI backend + Streamlit frontend with chat interface
- **Status**: ‚úÖ Complete & Validated
- **Details**: See `Sprint_4_Interactive_Interface/Sprint_4_COMPLETE.md`
- **Key Deliverables**:
  - ‚úÖ FastAPI backend with POST `/query` endpoint
  - ‚úÖ Pydantic validation for request/response models
  - ‚úÖ Streamlit chat interface with `st.chat_message`
  - ‚úÖ Source sidebar with colored badges (LOCAL=Green, NATIONAL=Blue)
  - ‚úÖ Expert Reasoning Trace expandable section
  - ‚úÖ Loading spinner during API processing
  - ‚úÖ NHS branding (title, subheader, colors)
  - ‚úÖ Clinical safety disclaimer footer (no duplication)
  - ‚úÖ `start_services.bat` and `run_app.py` scripts for deployment
  - ‚úÖ End-to-end query processing from UI to RAG engine
  - ‚úÖ **Prompt optimization: Subtractive structure (40% word reduction)**
  - ‚úÖ **Response quality: Accurate, concise, non-redundant (user validated)**
  - ‚úÖ **Fixed: qdrant-client version upgrade (1.16.0+) for hybrid search**
  - ‚úÖ **Fixed: Removed duplicate safety disclaimer from responses**

### Sprint 5: Evidence ‚úÖ
- **Goal**: Citation UI & Traceability - Visual Evidence for Clinical Trust
- **Deliverable**: Enhanced UI with dynamic source sidebar, PDF preview, Expert Trace dashboard, and audit trail logging
- **Status**: ‚úÖ Complete
- **Details**: See `Sprint_5_Evidence_Traceability/Sprint_5_COMPLETE.md`
- **Key Deliverables**:
  - ‚úÖ Dynamic source sidebar with source cards showing chunk text and confidence scores
  - ‚úÖ PDF preview integration with streamlit-pdf-viewer (view original documents)
  - ‚úÖ Expert Trace dashboard with detailed scoring breakdown (70% similarity, 20% priority, 10% recency)
  - ‚úÖ Audit trail logging system (logs/audit_trail.json) for Sprint 7 evaluation
  - ‚úÖ Enhanced API response models with detailed scoring (original_score, priority_score, recency_score)
  - ‚úÖ Source cards with document title, authority badge, chunk text, and confidence scores
  - ‚úÖ Technical trace toggle showing deterministic scoring math
  - ‚úÖ PDF viewer with slide number extraction for presentations
  - ‚úÖ Foundation for RAGAS evaluation in Sprint 7

### Sprint 6: API ‚è≥
- **Goal**: Clinical Trials Integration
- **Deliverable**: Agentic tool for live research fetching
- **Status**: ‚è≥ Pending
- **Key Components**:
  - [ ] ClinicalTrials.gov API integration
  - [ ] Dynamic trial fetching for UK/Diabetes
  - [ ] Agent tool implementation

### Sprint 7: Evaluation ‚úÖ
- **Goal**: RAGAS Testing - Quantify RAG System Accuracy
- **Deliverable**: Comprehensive evaluation system with 10 golden questions, RAGAS metrics, baseline results, and Windows compatibility
- **Status**: ‚úÖ Complete - Full 10-question evaluation executed with baseline metrics established
- **Details**: See `Sprint_7_Evaluation/Sprint_7_COMPLETE.md`
- **Key Deliverables**:
  - ‚úÖ RAGAS framework integration (ragas==0.4.2 with Langchain wrappers)
  - ‚úÖ Three key metrics implemented: **Faithfulness (0.58)**, **Answer Relevancy (0.84)**, **Context Precision (0.10)**
  - ‚úÖ 10 golden questions covering all policy domains (drugs, technology, patient rights, IFR)
  - ‚úÖ Full evaluation completed: 10/10 questions, 30/30 metrics, ~4 minutes processing time
  - ‚úÖ Ground truth answers manually crafted and validated against policy documents
  - ‚úÖ Evaluation script (`scripts/evaluate_rag.py`, 231 lines) with Windows compatibility fixes
  - ‚úÖ Results logging to `logs/ragas_evaluation_results.json` with per-question breakdown
  - ‚úÖ Terminal summary with ASCII status indicators ([OK] ‚â•0.85, [!] ‚â•0.70, [X] <0.70)
  - ‚úÖ Windows SSL optimization (Qdrant + OpenAI clients), Unicode encoding fixes, async event loop fixes
- **Baseline Results**:
  - Answer Relevancy: **0.84** ‚úÖ (Target: >0.70) - **MEETS TARGET**
  - Faithfulness: **0.58** ‚ùå (Target: >0.70) - Needs improvement (hallucination/extrapolation)
  - Context Precision: **0.10** ‚ùå (Target: >0.85) - **CRITICAL** - Retrieval pipeline bottleneck
  - Overall Average: **0.50** (Target: >0.75) - Optimization needed
- **Critical Findings**:
  - ‚ö†Ô∏è Context Precision (0.10) is primary bottleneck - retrieval system only finding ~10% of relevant info
  - ‚ö†Ô∏è Faithfulness (0.58) indicates hallucination - system prompt needs strengthening
  - ‚úÖ Answer Relevancy (0.84) is strong - query understanding works well

### Sprint 8: Polish ‚è≥
- **Goal**: Final UI & Documentation
- **Deliverable**: Portfolio-ready README and NHS-styled Dashboard
- **Status**: ‚è≥ Pending
- **Key Components**:
  - [ ] NHS Digital Design System styling
  - [ ] Comprehensive README
  - [ ] Architecture diagrams
  - [ ] Final UI polish

---

## Budget Tracking

- **Total Budget**: $10 (Contingency)
- **API Costs (Target)**: <$5 (GPT-4o-mini)
- **Infrastructure**: $0 (Local development, Streamlit Community Cloud)

**Current Spend**: ~$0.01 (Sprint 2 - OpenAI embeddings for 761 chunks, text-embedding-3-small)
- **Sprint 3**: Minimal cost (OpenAI query expansion + GPT-3.5-turbo responses, test queries only)
- **Sprint 4**: Minimal cost (GPT-3.5-turbo responses for user testing and validation)
- **Sprint 5**: No additional API costs (UI enhancements only, uses existing infrastructure)
- **Sprint 7**: Minimal cost (GPT-3.5-turbo for RAGAS metric calculations, ~30 API calls per evaluation run)

---

## Timeline

- **Start Date**: [To be updated]
- **Target Completion**: 2 weeks from start
- **Current Sprint**: Sprint 7 (Complete) ‚Üí Sprint 8 (Next - Final UI & Documentation)

---

## Key Success Metrics

- **Faithfulness Score (RAGAS)**: ‚ö†Ô∏è Target >0.70, Achieved 0.58 (Sprint 7) - Baseline established, needs optimization
- **Answer Relevancy (RAGAS)**: ‚úÖ Target >0.70, Achieved 0.84 (Sprint 7) - **EXCEEDS TARGET**
- **Context Precision (RAGAS)**: ‚ùå Target >0.85, Achieved 0.10 (Sprint 7) - **CRITICAL** - Retrieval optimization needed
- **Overall RAGAS Score**: ‚ö†Ô∏è Target >0.75, Achieved 0.50 (Sprint 7) - System functional but needs tuning
- **Citation Precision**: ‚úÖ Target 100% (Sprint 5) - Achieved with source cards, chunk text, and PDF preview
- **Response Latency**: ‚úÖ Achieved <5s (Sprint 4) - meets requirement
- **Response Quality**: ‚úÖ Accurate, concise responses validated (Sprint 4)

---

## Notes

- All code follows modular monolith pattern (code in `src/`, documentation in `Sprints/`)
- Python 3.11+ required
- All dependencies managed incrementally per sprint
- Medical safety: Local ICB Policy > National NICE Guidelines (critical rule)

---

**Last Updated**: After Sprint 7 full evaluation (10 questions, baseline metrics established) - January 2026

